 Network airflow_default  Creating
 Network airflow_default  Created
 Volume "airflow_postgres-db-volume"  Creating
 Volume "airflow_postgres-db-volume"  Created
 Container airflow-redis-1  Creating
 Container airflow-postgres-1  Creating
 Container airflow-redis-1  Created
 Container airflow-postgres-1  Created
 Container airflow-airflow-init-1  Creating
 Container airflow-airflow-init-1  Created
 Container airflow-airflow-webserver-1  Creating
 Container airflow-airflow-scheduler-1  Creating
 Container airflow-airflow-worker-1  Creating
 Container airflow-airflow-triggerer-1  Creating
 Container airflow-airflow-webserver-1  Created
 Container airflow-airflow-scheduler-1  Created
 Container airflow-airflow-worker-1  Created
 Container airflow-airflow-triggerer-1  Created
 Container airflow-redis-1  Starting
 Container airflow-postgres-1  Starting
 Container airflow-redis-1  Started
 Container airflow-postgres-1  Started
 Container airflow-postgres-1  Waiting
 Container airflow-redis-1  Waiting
 Container airflow-postgres-1  Healthy
 Container airflow-redis-1  Healthy
 Container airflow-airflow-init-1  Starting
 Container airflow-airflow-init-1  Started
 Container airflow-postgres-1  Waiting
 Container airflow-airflow-init-1  Waiting
 Container airflow-redis-1  Waiting
 Container airflow-redis-1  Waiting
 Container airflow-postgres-1  Waiting
 Container airflow-postgres-1  Waiting
 Container airflow-postgres-1  Waiting
 Container airflow-airflow-init-1  Waiting
 Container airflow-redis-1  Waiting
 Container airflow-airflow-init-1  Waiting
 Container airflow-redis-1  Waiting
 Container airflow-airflow-init-1  Waiting
 Container airflow-redis-1  Healthy
 Container airflow-postgres-1  Healthy
 Container airflow-postgres-1  Healthy
 Container airflow-postgres-1  Healthy
 Container airflow-redis-1  Healthy
 Container airflow-redis-1  Healthy
 Container airflow-redis-1  Healthy
 Container airflow-postgres-1  Healthy
 Container airflow-airflow-init-1  Exited
 Container airflow-airflow-triggerer-1  Starting
 Container airflow-airflow-init-1  Exited
 Container airflow-airflow-webserver-1  Starting
 Container airflow-airflow-init-1  Exited
 Container airflow-airflow-scheduler-1  Starting
 Container airflow-airflow-init-1  Exited
 Container airflow-airflow-worker-1  Starting
 Container airflow-airflow-worker-1  Started
 Container airflow-airflow-scheduler-1  Started
 Container airflow-airflow-webserver-1  Started
 Container airflow-airflow-triggerer-1  Started
Configuring pytest...
Setting up the model
Initializing Airflow
============================= test session starts ==============================
platform darwin -- Python 3.11.6, pytest-8.2.1, pluggy-1.5.0 -- /Users/vikram/Documents/ArdentAI/DE-Bench/dev_venv/bin/python3.11
cachedir: .pytest_cache
rootdir: /Users/vikram/Documents/ArdentAI/DE-Bench
configfile: pytest.ini
testpaths: Tests
plugins: anyio-4.8.0, xdist-3.6.1
collecting ... collected 5 items

Tests/Add_Record_To_MongoDB/test_add_record_to_mongodb.py::test_add_mongodb_record PASSED
Tests/Amazon_SP_API_To_Postgres/test_amazon_sp_api_to_postgres.py::test_amazon_sp_api_to_postgres Using repo format: ArdentAI1/Airflow-Test
Starting cleanup...
Paused the DAG
Branch might not exist or other error: 404 {"message": "Not Found", "documentation_url": "https://docs.github.com/rest/git/refs#get-all-references-in-a-namespace", "status": "404"}
Cleaned dags folder
Cleanup completed successfully
FAILED
Tests/MySQL_to_tigerbeetle/test_mysql_to_tigerbeetle.py::test_mysql_to_tigerbeetle time="2025-03-05T22:17:42-08:00" level=warning msg="/Users/vikram/Documents/ArdentAI/DE-Bench/Tests/MySQL_to_tigerbeetle/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Volume mysql_to_tigerbeetle_tigerbeetle_data  Removing
 Volume mysql_to_tigerbeetle_tigerbeetle_data  Removed
time="2025-03-05T22:17:44-08:00" level=warning msg="/Users/vikram/Documents/ArdentAI/DE-Bench/Tests/MySQL_to_tigerbeetle/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network mysql_to_tigerbeetle_default  Creating
 Network mysql_to_tigerbeetle_default  Created
 Volume "mysql_to_tigerbeetle_tigerbeetle_data"  Creating
 Volume "mysql_to_tigerbeetle_tigerbeetle_data"  Created
 Container mysql_to_tigerbeetle-setup-1  Creating
 Container mysql_to_tigerbeetle-setup-1  Created
 Container tigerbeetle  Creating
 Container tigerbeetle  Created
 Container mysql_to_tigerbeetle-setup-1  Starting
 Container mysql_to_tigerbeetle-setup-1  Started
 Container mysql_to_tigerbeetle-setup-1  Waiting
 Container mysql_to_tigerbeetle-setup-1  Exited
 Container tigerbeetle  Starting
 Container tigerbeetle  Started
time="2025-03-05T22:23:51-08:00" level=warning msg="/Users/vikram/Documents/ArdentAI/DE-Bench/Tests/MySQL_to_tigerbeetle/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container tigerbeetle  Stopping
 Container tigerbeetle  Stopped
 Container tigerbeetle  Removing
 Container tigerbeetle  Removed
 Container mysql_to_tigerbeetle-setup-1  Stopping
 Container mysql_to_tigerbeetle-setup-1  Stopped
 Container mysql_to_tigerbeetle-setup-1  Removing
 Container mysql_to_tigerbeetle-setup-1  Removed
 Volume mysql_to_tigerbeetle_tigerbeetle_data  Removing
 Network mysql_to_tigerbeetle_default  Removing
 Volume mysql_to_tigerbeetle_tigerbeetle_data  Removed
 Network mysql_to_tigerbeetle_default  Removed
Performing pre-cleanup...
Volume doesn't exist, which is fine
Pre-cleanup completed
Using repo format: ArdentAI1/Airflow-Test
DAG not found after max retries, checking for import errors...
[{'filename': '/opt/airflow/dags/mysql_to_tigerbeetle.py', 'import_error_id': 1, 'stack_trace': 'Traceback (most recent call last):\n  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed\n  File "/opt/airflow/dags/mysql_to_tigerbeetle.py", line 6, in <module>\n    import tigerbeetle as tb\nModuleNotFoundError: No module named \'tigerbeetle\'\n', 'timestamp': '2025-03-06T06:23:51.538934+00:00'}]
[{'filename': '/opt/airflow/dags/mysql_to_tigerbeetle.py', 'import_error_id': 1, 'stack_trace': 'Traceback (most recent call last):\n  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed\n  File "/opt/airflow/dags/mysql_to_tigerbeetle.py", line 6, in <module>\n    import tigerbeetle as tb\nModuleNotFoundError: No module named \'tigerbeetle\'\n', 'timestamp': '2025-03-06T06:23:51.538934+00:00'}]
DAG failed to load with import error: Traceback (most recent call last):
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/mysql_to_tigerbeetle.py", line 6, in <module>
    import tigerbeetle as tb
ModuleNotFoundError: No module named 'tigerbeetle'

Paused the DAG
Performing cleanup...
Cleaning up Docker containers and volumes...
Volume doesn't exist, which is fine
Cleanup completed
Deleted feature branch
Cleaned dags folder
FAILED
Tests/Postgres_to_MySQL_Pipeline/test_postgres_to_mysql_pipeline.py::test_postgres_to_mysql_pipeline Using repo format: ArdentAI1/Airflow-Test
GitHub repository: ArdentAI1/Airflow-Test
Cleaned dags folder.
Found connections to sales_db: []
Terminated all connections to sales_db
Dropped existing sales_db if it existed
Created new sales_db
Checking if the DAG exists...
Unpausing the DAG...
Triggering the DAG...
DAG triggered successfully
Monitoring the DAG run...
DAG state: queued
DAG state: success
DAG completed successfully
Verifying the outcomes...
Starting cleanup...
Paused the DAG
Closed test connections
Found connections to sales_db during cleanup: []
Terminated all connections to sales_db
Dropped sales_db in cleanup
Cleanup completed successfully
MySQL cleanup completed
Deleted feature branch
Cleaned dags folder
FAILED
Tests/Simple_Airflow_Pipeline/test_simple_airflow_pipeline.py::test_simple_airflow_pipeline Branch might not exist or other error: 404 {"message": "Not Found", "documentation_url": "https://docs.github.com/rest/git/refs#get-all-references-in-a-namespace", "status": "404"}
FAILED Container airflow-airflow-triggerer-1  Stopping
 Container airflow-airflow-scheduler-1  Stopping
 Container airflow-airflow-worker-1  Stopping
 Container airflow-airflow-webserver-1  Stopping
 Container airflow-airflow-triggerer-1  Stopped
 Container airflow-airflow-triggerer-1  Removing
 Container airflow-airflow-triggerer-1  Removed
 Container airflow-airflow-scheduler-1  Stopped
 Container airflow-airflow-scheduler-1  Removing
 Container airflow-airflow-scheduler-1  Removed
 Container airflow-airflow-worker-1  Stopped
 Container airflow-airflow-worker-1  Removing
 Container airflow-airflow-worker-1  Removed
 Container airflow-airflow-webserver-1  Stopped
 Container airflow-airflow-webserver-1  Removing
 Container airflow-airflow-webserver-1  Removed
 Container airflow-airflow-init-1  Stopping
 Container airflow-airflow-init-1  Stopped
 Container airflow-airflow-init-1  Removing
 Container airflow-airflow-init-1  Removed
 Container airflow-postgres-1  Stopping
 Container airflow-redis-1  Stopping
 Container airflow-postgres-1  Stopped
 Container airflow-postgres-1  Removing
 Container airflow-postgres-1  Removed
 Container airflow-redis-1  Stopped
 Container airflow-redis-1  Removing
 Container airflow-redis-1  Removed
 Volume airflow_postgres-db-volume  Removing
 Network airflow_default  Removing
 Volume airflow_postgres-db-volume  Removed
 Network airflow_default  Removed


=================================== FAILURES ===================================
________________________ test_amazon_sp_api_to_postgres ________________________

request = <FixtureRequest for <Function test_amazon_sp_api_to_postgres>>

    @pytest.mark.airflow
    @pytest.mark.postgres
    @pytest.mark.amazon_sp_api
    @pytest.mark.pipeline
    @pytest.mark.api_integration
    def test_amazon_sp_api_to_postgres(request):
        input_dir = os.path.dirname(os.path.abspath(__file__))
        request.node.user_properties.append(("user_query", Test_Configs.User_Input))
    
        test_steps = [
            {
                "name": "Checking Git Branch Existence",
                "description": "Checking if the git branch exists with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking PR Creation",
                "description": "Checking if the PR was created with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking DAG Results",
                "description": "Checking if the DAG correctly transforms and stores data",
                "status": "did not reach",
                "Result_Message": "",
            },
        ]
    
        request.node.user_properties.append(("test_steps", test_steps))
        config_results = None
        airflow_local = Airflow_Local()
    
        # SECTION 1: SETUP THE TEST
        try:
            # Setup GitHub repository with empty dags folder
            access_token = os.getenv("AIRFLOW_GITHUB_TOKEN")
            airflow_github_repo = os.getenv("AIRFLOW_REPO")
    
            # Convert full URL to owner/repo format if needed
            if "github.com" in airflow_github_repo:
                parts = airflow_github_repo.split("/")
                airflow_github_repo = f"{parts[-2]}/{parts[-1]}"
    
            print("Using repo format:", airflow_github_repo)
            g = Github(access_token)
            repo = g.get_repo(airflow_github_repo)
    
            # Clean up dags folder
            try:
                dags_contents = repo.get_contents("dags")
                for content in dags_contents:
                    if content.name != ".gitkeep":
                        repo.delete_file(
                            path=content.path,
                            message="Clear dags folder",
                            sha=content.sha,
                            branch="main",
                        )
    
                # Ensure .gitkeep exists
                try:
                    repo.get_contents("dags/.gitkeep")
                except:
                    repo.create_file(
                        path="dags/.gitkeep",
                        message="Add .gitkeep to dags folder",
                        content="",
                        branch="main",
                    )
            except Exception as e:
                if "sha" not in str(e):
                    raise e
    
            # Setup Postgres database
            postgres_connection = psycopg2.connect(
                host=os.getenv("POSTGRES_HOSTNAME"),
                port=os.getenv("POSTGRES_PORT"),
                user=os.getenv("POSTGRES_USERNAME"),
                password=os.getenv("POSTGRES_PASSWORD"),
                database="postgres",
                sslmode="require",
            )
            postgres_connection.autocommit = True
            postgres_cursor = postgres_connection.cursor()
    
            # Drop and recreate amazon_sales database
            postgres_cursor.execute(
                """
                SELECT pg_terminate_backend(pid)
                FROM pg_stat_activity
                WHERE datname = 'amazon_sales'
            """
            )
            postgres_cursor.execute("DROP DATABASE IF EXISTS amazon_sales")
            postgres_cursor.execute("CREATE DATABASE amazon_sales")
    
            # Close connection and reconnect to new database
            postgres_cursor.close()
            postgres_connection.close()
    
            postgres_connection = psycopg2.connect(
                host=os.getenv("POSTGRES_HOSTNAME"),
                port=os.getenv("POSTGRES_PORT"),
                user=os.getenv("POSTGRES_USERNAME"),
                password=os.getenv("POSTGRES_PASSWORD"),
                database="amazon_sales",
                sslmode="require",
            )
            postgres_cursor = postgres_connection.cursor()
    
            # Configure model with necessary configs
            config_results = set_up_model_configs(Configs=Test_Configs.Configs)
    
            # SECTION 2: RUN THE MODEL
            start_time = time.time()
            model_result = run_model(
                container=None, task=Test_Configs.User_Input, configs=Test_Configs.Configs
            )
            end_time = time.time()
            request.node.user_properties.append(("model_runtime", end_time - start_time))
    
            #input("Model has run we are now checking the results")
            # Check if the branch exists
            try:
>               branch = repo.get_branch("feature/amazon_sp_api_pipeline")

Tests/Amazon_SP_API_To_Postgres/test_amazon_sp_api_to_postgres.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
dev_venv/lib/python3.11/site-packages/github/Repository.py:2056: in get_branch
    headers, data = self._requester.requestJsonAndCheck("GET", f"{self.url}/branches/{branch}")
dev_venv/lib/python3.11/site-packages/github/Requester.py:586: in requestJsonAndCheck
    return self.__check(*self.requestJson(verb, url, parameters, headers, input, self.__customConnection(url)))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <github.Requester.Requester object at 0x10fbefcd0>, status = 404
responseHeaders = {'access-control-allow-origin': '*', 'access-control-expose-headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP...Hub-Request-Id, Deprecation, Sunset', 'content-encoding': 'gzip', 'content-security-policy': "default-src 'none'", ...}
output = '{"message":"Branch not found","documentation_url":"https://docs.github.com/rest/branches/branches#get-a-branch","status":"404"}'

    def __check(
        self,
        status: int,
        responseHeaders: Dict[str, Any],
        output: str,
    ) -> Tuple[Dict[str, Any], Any]:
        data = self.__structuredFromJson(output)
        if status >= 400:
>           raise self.createException(status, responseHeaders, data)
E           github.GithubException.GithubException: 404 {"message": "Branch not found", "documentation_url": "https://docs.github.com/rest/branches/branches#get-a-branch", "status": "404"}

dev_venv/lib/python3.11/site-packages/github/Requester.py:744: GithubException

During handling of the above exception, another exception occurred:

request = <FixtureRequest for <Function test_amazon_sp_api_to_postgres>>

    @pytest.mark.airflow
    @pytest.mark.postgres
    @pytest.mark.amazon_sp_api
    @pytest.mark.pipeline
    @pytest.mark.api_integration
    def test_amazon_sp_api_to_postgres(request):
        input_dir = os.path.dirname(os.path.abspath(__file__))
        request.node.user_properties.append(("user_query", Test_Configs.User_Input))
    
        test_steps = [
            {
                "name": "Checking Git Branch Existence",
                "description": "Checking if the git branch exists with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking PR Creation",
                "description": "Checking if the PR was created with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking DAG Results",
                "description": "Checking if the DAG correctly transforms and stores data",
                "status": "did not reach",
                "Result_Message": "",
            },
        ]
    
        request.node.user_properties.append(("test_steps", test_steps))
        config_results = None
        airflow_local = Airflow_Local()
    
        # SECTION 1: SETUP THE TEST
        try:
            # Setup GitHub repository with empty dags folder
            access_token = os.getenv("AIRFLOW_GITHUB_TOKEN")
            airflow_github_repo = os.getenv("AIRFLOW_REPO")
    
            # Convert full URL to owner/repo format if needed
            if "github.com" in airflow_github_repo:
                parts = airflow_github_repo.split("/")
                airflow_github_repo = f"{parts[-2]}/{parts[-1]}"
    
            print("Using repo format:", airflow_github_repo)
            g = Github(access_token)
            repo = g.get_repo(airflow_github_repo)
    
            # Clean up dags folder
            try:
                dags_contents = repo.get_contents("dags")
                for content in dags_contents:
                    if content.name != ".gitkeep":
                        repo.delete_file(
                            path=content.path,
                            message="Clear dags folder",
                            sha=content.sha,
                            branch="main",
                        )
    
                # Ensure .gitkeep exists
                try:
                    repo.get_contents("dags/.gitkeep")
                except:
                    repo.create_file(
                        path="dags/.gitkeep",
                        message="Add .gitkeep to dags folder",
                        content="",
                        branch="main",
                    )
            except Exception as e:
                if "sha" not in str(e):
                    raise e
    
            # Setup Postgres database
            postgres_connection = psycopg2.connect(
                host=os.getenv("POSTGRES_HOSTNAME"),
                port=os.getenv("POSTGRES_PORT"),
                user=os.getenv("POSTGRES_USERNAME"),
                password=os.getenv("POSTGRES_PASSWORD"),
                database="postgres",
                sslmode="require",
            )
            postgres_connection.autocommit = True
            postgres_cursor = postgres_connection.cursor()
    
            # Drop and recreate amazon_sales database
            postgres_cursor.execute(
                """
                SELECT pg_terminate_backend(pid)
                FROM pg_stat_activity
                WHERE datname = 'amazon_sales'
            """
            )
            postgres_cursor.execute("DROP DATABASE IF EXISTS amazon_sales")
            postgres_cursor.execute("CREATE DATABASE amazon_sales")
    
            # Close connection and reconnect to new database
            postgres_cursor.close()
            postgres_connection.close()
    
            postgres_connection = psycopg2.connect(
                host=os.getenv("POSTGRES_HOSTNAME"),
                port=os.getenv("POSTGRES_PORT"),
                user=os.getenv("POSTGRES_USERNAME"),
                password=os.getenv("POSTGRES_PASSWORD"),
                database="amazon_sales",
                sslmode="require",
            )
            postgres_cursor = postgres_connection.cursor()
    
            # Configure model with necessary configs
            config_results = set_up_model_configs(Configs=Test_Configs.Configs)
    
            # SECTION 2: RUN THE MODEL
            start_time = time.time()
            model_result = run_model(
                container=None, task=Test_Configs.User_Input, configs=Test_Configs.Configs
            )
            end_time = time.time()
            request.node.user_properties.append(("model_runtime", end_time - start_time))
    
            #input("Model has run we are now checking the results")
            # Check if the branch exists
            try:
                branch = repo.get_branch("feature/amazon_sp_api_pipeline")
                test_steps[0]["status"] = "passed"
                test_steps[0]["Result_Message"] = "Branch 'feature/amazon_sp_api_pipeline' was created successfully"
            except Exception as e:
                test_steps[0]["status"] = "failed"
                test_steps[0]["Result_Message"] = f"Branch 'feature/amazon_sp_api_pipeline' was not created: {str(e)}"
>               raise Exception(f"Branch 'feature/amazon_sp_api_pipeline' was not created: {str(e)}")
E               Exception: Branch 'feature/amazon_sp_api_pipeline' was not created: 404 {"message": "Branch not found", "documentation_url": "https://docs.github.com/rest/branches/branches#get-a-branch", "status": "404"}

Tests/Amazon_SP_API_To_Postgres/test_amazon_sp_api_to_postgres.py:154: Exception
__________________________ test_mysql_to_tigerbeetle ___________________________

request = <FixtureRequest for <Function test_mysql_to_tigerbeetle>>

    @pytest.mark.airflow
    @pytest.mark.mysql
    @pytest.mark.tigerbeetle
    @pytest.mark.plaid
    @pytest.mark.finch
    @pytest.mark.api_integration
    @pytest.mark.database
    @pytest.mark.pipeline
    def test_mysql_to_tigerbeetle(request):
        input_dir = os.path.dirname(os.path.abspath(__file__))
        request.node.user_properties.append(("user_query", Test_Configs.User_Input))
    
        test_steps = [
            {
                "name": "Checking Git Branch Existence",
                "description": "Checking if the git branch exists with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking PR Creation",
                "description": "Checking if the PR was created with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking DAG Results",
                "description": "Checking if the DAG correctly transforms and stores data",
                "status": "did not reach",
                "Result_Message": "",
            },
        ]
    
        request.node.user_properties.append(("test_steps", test_steps))
    
        # Create a Docker client with the compose file configuration
        docker = DockerClient(compose_files=[os.path.join(input_dir, "docker-compose.yml")])
        config_results = None
        airflow_local = Airflow_Local()
        cursor = connection.cursor()
    
    
    
        # Pre-cleanup to ensure we start fresh
        try:
            print("Performing pre-cleanup...")
            # Force down any existing containers and remove volumes
            docker.compose.down(volumes=True)
    
            # Additional cleanup for any orphaned volumes using Python on Whales
            try:
                # Try without force parameter
                docker.volume.remove("mysql_to_tigerbeetle_tigerbeetle_data")
                print("Removed tigerbeetle volume")
            except NoSuchVolume:
                print("Volume doesn't exist, which is fine")
            except Exception as vol_err:
                print(f"Other error when removing volume: {vol_err}")
    
            print("Pre-cleanup completed")
        except Exception as e:
            print(f"Error during pre-cleanup: {e}")
    
        # SECTION 1: SETUP THE TEST
        try:
            # Setup GitHub repository with empty dags folder
            access_token = os.getenv("AIRFLOW_GITHUB_TOKEN")
            airflow_github_repo = os.getenv("AIRFLOW_REPO")
    
            # Convert full URL to owner/repo format if needed
            if "github.com" in airflow_github_repo:
                # Extract owner/repo from URL
                parts = airflow_github_repo.split("/")
                airflow_github_repo = f"{parts[-2]}/{parts[-1]}"
    
            print("Using repo format:", airflow_github_repo)
            g = Github(access_token)
            repo = g.get_repo(airflow_github_repo)
    
            try:
                # First, clear only the dags folder
                dags_contents = repo.get_contents("dags")
                for content in dags_contents:
                    if content.name != ".gitkeep":  # Keep the .gitkeep file if it exists
                        repo.delete_file(
                            path=content.path,
                            message="Clear dags folder",
                            sha=content.sha,
                            branch="main",
                        )
    
                # Ensure .gitkeep exists in dags folder
                try:
                    repo.get_contents("dags/.gitkeep")
                except:
                    repo.create_file(
                        path="dags/.gitkeep",
                        message="Add .gitkeep to dags folder",
                        content="",
                        branch="main",
                    )
            except Exception as e:
                if "sha" not in str(e):  # If error is not about folder already existing
                    raise e
    
            # Start docker-compose to set up tigerbeetle
            docker.compose.up(detach=True)
    
            # Give TigerBeetle a moment to start up
            time.sleep(10)
    
            # Set up model configs
    
            # Now we set up the MySQL Instance
    
            # Create a test database and then select it to execute the queries
            cursor.execute("CREATE DATABASE IF NOT EXISTS Access_Tokens")
            cursor.execute("USE Access_Tokens")
    
            # Create tables for Plaid and Finch access tokens
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS plaid_access_tokens (
                    company_id VARCHAR(50) PRIMARY KEY,
                    access_token VARCHAR(255) NOT NULL
                )
            """
            )
    
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS finch_access_tokens (
                    company_id VARCHAR(50) PRIMARY KEY,
                    access_token VARCHAR(255) NOT NULL
                )
            """
            )
    
            # Insert test data with IGNORE to skip duplicates
            cursor.execute(
                """
                INSERT IGNORE INTO plaid_access_tokens (company_id, access_token)
                VALUES (%s, %s)
            """,
                ("123", "test_plaid_token"),
            )
    
            cursor.execute(
                """
                INSERT IGNORE INTO finch_access_tokens (company_id, access_token)
                VALUES (%s, %s)
            """,
                ("123", "test_finch_token"),
            )
    
            connection.commit()
    
            config_results = set_up_model_configs(Configs=Test_Configs.Configs)
    
    
    
    
            # SECTION 2: RUN THE MODEL
            start_time = time.time()
            run_model(
                container=None, task=Test_Configs.User_Input, configs=Test_Configs.Configs
            )
            end_time = time.time()
            request.node.user_properties.append(("model_runtime", end_time - start_time))
    
            # Check if the branch exists
            try:
                branch = repo.get_branch("feature/mysql_to_tigerbeetle")
                test_steps[0]["status"] = "passed"
                test_steps[0]["Result_Message"] = "Branch 'feature/mysql_to_tigerbeetle' was created successfully"
            except Exception as e:
                test_steps[0]["status"] = "failed"
                test_steps[0]["Result_Message"] = f"Branch 'feature/mysql_to_tigerbeetle' was not created: {str(e)}"
                raise Exception(f"Branch 'feature/mysql_to_tigerbeetle' was not created: {str(e)}")
    
            # Find and merge the PR
            pulls = repo.get_pulls(state="open")
            target_pr = None
            for pr in pulls:
                if pr.title == "Add MySQL to TigerBeetle Pipeline":  # Look for PR by title
                    target_pr = pr
                    test_steps[1]["status"] = "passed"
                    test_steps[1]["Result_Message"] = "PR 'Add MySQL to TigerBeetle Pipeline' was created successfully"
                    break
    
            if not target_pr:
                test_steps[1]["status"] = "failed"
                test_steps[1]["Result_Message"] = "PR 'Add MySQL to TigerBeetle Pipeline' not found"
                raise Exception("PR 'Add MySQL to TigerBeetle Pipeline' not found")
    
            # Merge the PR
            merge_result = target_pr.merge(
                commit_title="Add MySQL to TigerBeetle Pipeline", merge_method="squash"
            )
    
            if not merge_result.merged:
                raise Exception(f"Failed to merge PR: {merge_result.message}")
    
            #input("Prior to dag fetch. We should have merged the PR...")
    
            # Get the DAGs from GitHub
            airflow_local.Get_Airflow_Dags_From_Github()
    
            # After merging, wait for Airflow to detect changes
            time.sleep(10)  # Give Airflow time to scan for new DAGs
    
            #input("We should see the dags in the folder now...")
    
            # Trigger the DAG
            airflow_base_url = os.getenv("AIRFLOW_HOST")
            airflow_username = os.getenv("AIRFLOW_USERNAME")
            airflow_password = os.getenv("AIRFLOW_PASSWORD")
    
            # Wait for DAG to appear and trigger it
            max_retries = 5
            auth = HTTPBasicAuth(airflow_username, airflow_password)
            headers = {"Content-Type": "application/json", "Cache-Control": "no-cache"}
    
            for attempt in range(max_retries):
                # Check if DAG exists
                dag_response = requests.get(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/mysql_to_tigerbeetle",
                    auth=auth,
                    headers=headers,
                )
    
                if dag_response.status_code != 200:
                    if attempt == max_retries - 1:
    
                        # Check for import errors before giving up
                        print("DAG not found after max retries, checking for import errors...")
                        import_errors_response = requests.get(
                            f"{airflow_base_url.rstrip('/')}/api/v1/importErrors",
                            auth=auth,
                            headers=headers
                        )
    
                        if import_errors_response.status_code == 200:
                            import_errors = import_errors_response.json()['import_errors']
    
                            print(import_errors)
    
                            # Filter errors related to your specific DAG
                            dag_errors = [error for error in import_errors
                                         if "mysql_to_tigerbeetle.py" in error['filename']]
    
                            print(dag_errors)
    
                            if dag_errors:
                                error_message = f"DAG failed to load with import error: {dag_errors[0]['stack_trace']}"
                                print(error_message)
                                test_steps[2]["status"] = "failed"
                                test_steps[2]["Result_Message"] = error_message
>                               raise Exception("Dag error which caused dag to not load")
E                               Exception: Dag error which caused dag to not load

Tests/MySQL_to_tigerbeetle/test_mysql_to_tigerbeetle.py:283: Exception

During handling of the above exception, another exception occurred:

request = <FixtureRequest for <Function test_mysql_to_tigerbeetle>>

    @pytest.mark.airflow
    @pytest.mark.mysql
    @pytest.mark.tigerbeetle
    @pytest.mark.plaid
    @pytest.mark.finch
    @pytest.mark.api_integration
    @pytest.mark.database
    @pytest.mark.pipeline
    def test_mysql_to_tigerbeetle(request):
        input_dir = os.path.dirname(os.path.abspath(__file__))
        request.node.user_properties.append(("user_query", Test_Configs.User_Input))
    
        test_steps = [
            {
                "name": "Checking Git Branch Existence",
                "description": "Checking if the git branch exists with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking PR Creation",
                "description": "Checking if the PR was created with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking DAG Results",
                "description": "Checking if the DAG correctly transforms and stores data",
                "status": "did not reach",
                "Result_Message": "",
            },
        ]
    
        request.node.user_properties.append(("test_steps", test_steps))
    
        # Create a Docker client with the compose file configuration
        docker = DockerClient(compose_files=[os.path.join(input_dir, "docker-compose.yml")])
        config_results = None
        airflow_local = Airflow_Local()
        cursor = connection.cursor()
    
    
    
        # Pre-cleanup to ensure we start fresh
        try:
            print("Performing pre-cleanup...")
            # Force down any existing containers and remove volumes
            docker.compose.down(volumes=True)
    
            # Additional cleanup for any orphaned volumes using Python on Whales
            try:
                # Try without force parameter
                docker.volume.remove("mysql_to_tigerbeetle_tigerbeetle_data")
                print("Removed tigerbeetle volume")
            except NoSuchVolume:
                print("Volume doesn't exist, which is fine")
            except Exception as vol_err:
                print(f"Other error when removing volume: {vol_err}")
    
            print("Pre-cleanup completed")
        except Exception as e:
            print(f"Error during pre-cleanup: {e}")
    
        # SECTION 1: SETUP THE TEST
        try:
            # Setup GitHub repository with empty dags folder
            access_token = os.getenv("AIRFLOW_GITHUB_TOKEN")
            airflow_github_repo = os.getenv("AIRFLOW_REPO")
    
            # Convert full URL to owner/repo format if needed
            if "github.com" in airflow_github_repo:
                # Extract owner/repo from URL
                parts = airflow_github_repo.split("/")
                airflow_github_repo = f"{parts[-2]}/{parts[-1]}"
    
            print("Using repo format:", airflow_github_repo)
            g = Github(access_token)
            repo = g.get_repo(airflow_github_repo)
    
            try:
                # First, clear only the dags folder
                dags_contents = repo.get_contents("dags")
                for content in dags_contents:
                    if content.name != ".gitkeep":  # Keep the .gitkeep file if it exists
                        repo.delete_file(
                            path=content.path,
                            message="Clear dags folder",
                            sha=content.sha,
                            branch="main",
                        )
    
                # Ensure .gitkeep exists in dags folder
                try:
                    repo.get_contents("dags/.gitkeep")
                except:
                    repo.create_file(
                        path="dags/.gitkeep",
                        message="Add .gitkeep to dags folder",
                        content="",
                        branch="main",
                    )
            except Exception as e:
                if "sha" not in str(e):  # If error is not about folder already existing
                    raise e
    
            # Start docker-compose to set up tigerbeetle
            docker.compose.up(detach=True)
    
            # Give TigerBeetle a moment to start up
            time.sleep(10)
    
            # Set up model configs
    
            # Now we set up the MySQL Instance
    
            # Create a test database and then select it to execute the queries
            cursor.execute("CREATE DATABASE IF NOT EXISTS Access_Tokens")
            cursor.execute("USE Access_Tokens")
    
            # Create tables for Plaid and Finch access tokens
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS plaid_access_tokens (
                    company_id VARCHAR(50) PRIMARY KEY,
                    access_token VARCHAR(255) NOT NULL
                )
            """
            )
    
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS finch_access_tokens (
                    company_id VARCHAR(50) PRIMARY KEY,
                    access_token VARCHAR(255) NOT NULL
                )
            """
            )
    
            # Insert test data with IGNORE to skip duplicates
            cursor.execute(
                """
                INSERT IGNORE INTO plaid_access_tokens (company_id, access_token)
                VALUES (%s, %s)
            """,
                ("123", "test_plaid_token"),
            )
    
            cursor.execute(
                """
                INSERT IGNORE INTO finch_access_tokens (company_id, access_token)
                VALUES (%s, %s)
            """,
                ("123", "test_finch_token"),
            )
    
            connection.commit()
    
            config_results = set_up_model_configs(Configs=Test_Configs.Configs)
    
    
    
    
            # SECTION 2: RUN THE MODEL
            start_time = time.time()
            run_model(
                container=None, task=Test_Configs.User_Input, configs=Test_Configs.Configs
            )
            end_time = time.time()
            request.node.user_properties.append(("model_runtime", end_time - start_time))
    
            # Check if the branch exists
            try:
                branch = repo.get_branch("feature/mysql_to_tigerbeetle")
                test_steps[0]["status"] = "passed"
                test_steps[0]["Result_Message"] = "Branch 'feature/mysql_to_tigerbeetle' was created successfully"
            except Exception as e:
                test_steps[0]["status"] = "failed"
                test_steps[0]["Result_Message"] = f"Branch 'feature/mysql_to_tigerbeetle' was not created: {str(e)}"
                raise Exception(f"Branch 'feature/mysql_to_tigerbeetle' was not created: {str(e)}")
    
            # Find and merge the PR
            pulls = repo.get_pulls(state="open")
            target_pr = None
            for pr in pulls:
                if pr.title == "Add MySQL to TigerBeetle Pipeline":  # Look for PR by title
                    target_pr = pr
                    test_steps[1]["status"] = "passed"
                    test_steps[1]["Result_Message"] = "PR 'Add MySQL to TigerBeetle Pipeline' was created successfully"
                    break
    
            if not target_pr:
                test_steps[1]["status"] = "failed"
                test_steps[1]["Result_Message"] = "PR 'Add MySQL to TigerBeetle Pipeline' not found"
                raise Exception("PR 'Add MySQL to TigerBeetle Pipeline' not found")
    
            # Merge the PR
            merge_result = target_pr.merge(
                commit_title="Add MySQL to TigerBeetle Pipeline", merge_method="squash"
            )
    
            if not merge_result.merged:
                raise Exception(f"Failed to merge PR: {merge_result.message}")
    
            #input("Prior to dag fetch. We should have merged the PR...")
    
            # Get the DAGs from GitHub
            airflow_local.Get_Airflow_Dags_From_Github()
    
            # After merging, wait for Airflow to detect changes
            time.sleep(10)  # Give Airflow time to scan for new DAGs
    
            #input("We should see the dags in the folder now...")
    
            # Trigger the DAG
            airflow_base_url = os.getenv("AIRFLOW_HOST")
            airflow_username = os.getenv("AIRFLOW_USERNAME")
            airflow_password = os.getenv("AIRFLOW_PASSWORD")
    
            # Wait for DAG to appear and trigger it
            max_retries = 5
            auth = HTTPBasicAuth(airflow_username, airflow_password)
            headers = {"Content-Type": "application/json", "Cache-Control": "no-cache"}
    
            for attempt in range(max_retries):
                # Check if DAG exists
                dag_response = requests.get(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/mysql_to_tigerbeetle",
                    auth=auth,
                    headers=headers,
                )
    
                if dag_response.status_code != 200:
                    if attempt == max_retries - 1:
    
                        # Check for import errors before giving up
                        print("DAG not found after max retries, checking for import errors...")
                        import_errors_response = requests.get(
                            f"{airflow_base_url.rstrip('/')}/api/v1/importErrors",
                            auth=auth,
                            headers=headers
                        )
    
                        if import_errors_response.status_code == 200:
                            import_errors = import_errors_response.json()['import_errors']
    
                            print(import_errors)
    
                            # Filter errors related to your specific DAG
                            dag_errors = [error for error in import_errors
                                         if "mysql_to_tigerbeetle.py" in error['filename']]
    
                            print(dag_errors)
    
                            if dag_errors:
                                error_message = f"DAG failed to load with import error: {dag_errors[0]['stack_trace']}"
                                print(error_message)
                                test_steps[2]["status"] = "failed"
                                test_steps[2]["Result_Message"] = error_message
                                raise Exception("Dag error which caused dag to not load")
    
                        raise Exception("DAG not found after max retries")
                    time.sleep(10)
                    continue
    
                # Unpause the DAG before triggering
                unpause_response = requests.patch(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/mysql_to_tigerbeetle",
                    auth=auth,
                    headers=headers,
                    json={"is_paused": False},
                )
    
                if unpause_response.status_code != 200:
                    if attempt == max_retries - 1:
                        raise Exception(f"Failed to unpause DAG: {unpause_response.text}")
                    time.sleep(10)
                    continue
    
                # Trigger the DAG
                trigger_response = requests.post(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/mysql_to_tigerbeetle/dagRuns",
                    auth=auth,
                    headers=headers,
                    json={"conf": {}},
                )
    
                if trigger_response.status_code == 200:
                    dag_run_id = trigger_response.json()["dag_run_id"]
                    break
                else:
                    if attempt == max_retries - 1:
                        raise Exception(f"Failed to trigger DAG: {trigger_response.text}")
                    time.sleep(10)
    
            # Monitor the DAG run
            max_wait = 120  # 2 minutes timeout
            start_time = time.time()
            while time.time() - start_time < max_wait:
                status_response = requests.get(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/mysql_to_tigerbeetle/dagRuns/{dag_run_id}",
                    auth=auth,
                    headers=headers,
                )
    
                if status_response.status_code == 200:
                    state = status_response.json()["state"]
                    if state == "success":
                        break
                    elif state in ["failed", "error"]:
                        raise Exception(f"DAG failed with state: {state}")
    
                time.sleep(5)
            else:
                raise Exception("DAG run timed out")
    
            # SECTION 3: VERIFY THE OUTCOMES
            # In a real test, we would verify the data in TigerBeetle
            # For now, we'll consider the test successful if the DAG ran successfully
            test_steps[2]["status"] = "passed"
            test_steps[2]["Result_Message"] = "DAG executed successfully and data was transformed and stored in TigerBeetle"
    
        except Exception as e:
            for step in test_steps:
                if step["status"] == "did not reach":
                    step["status"] = "failed"
                    step["Result_Message"] = f"Error during test execution: {str(e)}"
>           raise Exception(f"Error during test execution: {e}")
E           Exception: Error during test execution: Dag error which caused dag to not load

Tests/MySQL_to_tigerbeetle/test_mysql_to_tigerbeetle.py:351: Exception
_______________________ test_postgres_to_mysql_pipeline ________________________

request = <FixtureRequest for <Function test_postgres_to_mysql_pipeline>>

    @pytest.mark.airflow
    @pytest.mark.postgres
    @pytest.mark.mysql
    @pytest.mark.pipeline
    @pytest.mark.database
    def test_postgres_to_mysql_pipeline(request):
        input_dir = os.path.dirname(os.path.abspath(__file__))
    
        # Create a Docker client with the compose file configuration
        airflow_local = Airflow_Local()
    
    
        test_steps = [
            {
                "name": "Checking Git Branch Existence",
                "description": "Checking if the git branch exists with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking PR Creation",
                "description": "Checking if the PR was created with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking Dag Results",
                "description": "Checking if the DAG produces the expected results",
                "status": "did not reach",
                "Result_Message": "",
            },
        ]
    
        request.node.user_properties.append(("test_steps", test_steps))
        request.node.user_properties.append(("user_query", Test_Configs.User_Input))
    
        config_results = None
    
        # SECTION 1: SETUP THE TEST
        try:
            # Setup GitHub repository with empty dags folder
            access_token = os.getenv("AIRFLOW_GITHUB_TOKEN")
            airflow_github_repo = os.getenv("AIRFLOW_REPO")
    
            # Convert full URL to owner/repo format if needed
            if "github.com" in airflow_github_repo:
                # Extract owner/repo from URL
                parts = airflow_github_repo.split("/")
                airflow_github_repo = f"{parts[-2]}/{parts[-1]}"
    
            print("Using repo format:", airflow_github_repo)
            g = Github(access_token)
            repo = g.get_repo(airflow_github_repo)
            main_branch = repo.get_branch("main")
    
            print("GitHub repository:", airflow_github_repo)
    
            try:
                # First, clear only the dags folder
                dags_contents = repo.get_contents("dags")
                for content in dags_contents:
                    if content.name != ".gitkeep":  # Keep the .gitkeep file if it exists
                        repo.delete_file(
                            path=content.path,
                            message="Clear dags folder",
                            sha=content.sha,
                            branch="main",
                        )
    
                # Ensure .gitkeep exists in dags folder
                try:
                    repo.get_contents("dags/.gitkeep")
                except:
                    repo.create_file(
                        path="dags/.gitkeep",
                        message="Add .gitkeep to dags folder",
                        content="",
                        branch="main",
                    )
                print("Cleaned dags folder.")
    
            except Exception as e:
                if "sha" not in str(e):  # If error is not about folder already existing
                    raise e
    
            # Setup Postgres database and sample data
            connection = psycopg2.connect(
                host=os.getenv("POSTGRES_HOSTNAME"),
                port=os.getenv("POSTGRES_PORT"),
                user=os.getenv("POSTGRES_USERNAME"),
                password=os.getenv("POSTGRES_PASSWORD"),
                database="postgres",
                sslmode="require",
            )
            postgres_cursor = connection.cursor()
    
            # First connect to postgres database
            postgres_connection = psycopg2.connect(
                host=os.getenv("POSTGRES_HOSTNAME"),
                port=os.getenv("POSTGRES_PORT"),
                user=os.getenv("POSTGRES_USERNAME"),
                password=os.getenv("POSTGRES_PASSWORD"),
                database="postgres",
                sslmode="require",
            )
            postgres_connection.autocommit = True
            postgres_cursor = postgres_connection.cursor()
    
            # Check and kill any existing connections
            postgres_cursor.execute(
                """
                SELECT pid, usename, datname
                FROM pg_stat_activity
                WHERE datname = 'sales_db'
            """
            )
            connections = postgres_cursor.fetchall()
            print(f"Found connections to sales_db:", connections)
    
            postgres_cursor.execute(
                """
                SELECT pg_terminate_backend(pid)
                FROM pg_stat_activity
                WHERE datname = 'sales_db'
            """
            )
            print("Terminated all connections to sales_db")
    
            # Now safe to drop and recreate
            postgres_cursor.execute("DROP DATABASE IF EXISTS sales_db")
            print("Dropped existing sales_db if it existed")
            postgres_cursor.execute("CREATE DATABASE sales_db")
            print("Created new sales_db")
    
            # Close connection to postgres database
            postgres_cursor.close()
            postgres_connection.close()
    
            # Reconnect to the new database
            postgres_connection = psycopg2.connect(
                host=os.getenv("POSTGRES_HOSTNAME"),
                port=os.getenv("POSTGRES_PORT"),
                user=os.getenv("POSTGRES_USERNAME"),
                password=os.getenv("POSTGRES_PASSWORD"),
                database="sales_db",
                sslmode="require",
            )
            postgres_cursor = postgres_connection.cursor()
    
            # Create test table
            postgres_cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS transactions (
                    transaction_id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL,
                    product_id INTEGER NOT NULL,
                    sale_amount DECIMAL(10,2) NOT NULL,
                    cost_amount DECIMAL(10,2) NOT NULL,
                    transaction_date DATE NOT NULL
                )
            """
            )
    
            # Insert sample data
            postgres_cursor.execute(
                """
                INSERT INTO transactions
                (user_id, product_id, sale_amount, cost_amount, transaction_date)
                VALUES
                (1, 101, 100.00, 60.00, '2024-01-01'),
                (1, 102, 150.00, 90.00, '2024-01-01'),
                (2, 101, 200.00, 120.00, '2024-01-01')
            """
            )
    
            # Make sure to commit the transaction
            postgres_connection.commit()
    
            # Verify the insert
            postgres_cursor.execute("SELECT * FROM transactions")
            data = postgres_cursor.fetchall()
    
            # Setup MySQL database
            mysql_cursor = mysql_connection.cursor()
            mysql_cursor.execute("CREATE DATABASE IF NOT EXISTS analytics_db")
            mysql_cursor.execute("USE analytics_db")
    
            # Create result table
            mysql_cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS daily_profits (
                    date DATE,
                    user_id INTEGER,
                    total_sales DECIMAL(10,2),
                    total_costs DECIMAL(10,2),
                    total_profit DECIMAL(10,2),
                    PRIMARY KEY (date, user_id)
                )
            """
            )
    
            mysql_connection.commit()
    
            # Verify table creation
            mysql_cursor.execute("SHOW TABLES")
            tables = mysql_cursor.fetchall()
    
            # Verify table structure
            mysql_cursor.execute("DESCRIBE daily_profits")
            structure = mysql_cursor.fetchall()
    
            # Configure Ardent with database connections
            config_results = set_up_model_configs(Test_Configs.Configs)
    
            # SECTION 2: RUN THE MODEL
            start_time = time.time()
            run_model(
                container=None, task=Test_Configs.User_Input, configs=Test_Configs.Configs
            )
            end_time = time.time()
            request.node.user_properties.append(("model_runtime", end_time - start_time))
    
            time.sleep(10)
    
            # Check if the branch exists
            try:
                branch = repo.get_branch("feature/sales_profit_pipeline")
                test_steps[0]["status"] = "passed"
                test_steps[0][
                    "Result_Message"
                ] = "Branch 'feature/sales_profit_pipeline' was created successfully"
            except Exception as e:
                test_steps[0]["status"] = "failed"
                test_steps[0][
                    "Result_Message"
                ] = f"Branch 'feature/sales_profit_pipeline' was not created: {str(e)}"
                raise Exception(
                    f"Branch 'feature/sales_profit_pipeline' was not created: {str(e)}"
                )
    
            # After model creates the PR, find and merge it
            pulls = repo.get_pulls(state="open")
            target_pr = None
            for pr in pulls:
                if pr.title == "Merge_Sales_Profit_Pipeline":
                    target_pr = pr
                    test_steps[1]["status"] = "passed"
                    test_steps[1][
                        "Result_Message"
                    ] = "PR 'Merge_Sales_Profit_Pipeline' was created successfully"
                    break
    
            if not target_pr:
                test_steps[1]["status"] = "failed"
                test_steps[1][
                    "Result_Message"
                ] = "PR 'Merge_Sales_Profit_Pipeline' not found"
                raise Exception("PR 'Merge_Sales_Profit_Pipeline' not found")
    
            # Merge the PR
            merge_result = target_pr.merge(
                commit_title="Merge_Sales_Profit_Pipeline", merge_method="squash"
            )
    
            if not merge_result.merged:
                raise Exception(f"Failed to merge PR: {merge_result.message}")
    
            # now we run the function to get the dag
            airflow_local.Get_Airflow_Dags_From_Github()
    
            # After creating the DAG, wait a bit for Airflow to detect it
            time.sleep(5)  # Give Airflow time to scan for new DAGs
    
            # Trigger the DAG
            airflow_base_url = os.getenv("AIRFLOW_HOST")
            airflow_username = os.getenv("AIRFLOW_USERNAME")
            airflow_password = os.getenv("AIRFLOW_PASSWORD")
    
            # Wait for DAG to appear and trigger it
            max_retries = 5
            auth = HTTPBasicAuth(airflow_username, airflow_password)
            headers = {"Content-Type": "application/json", "Cache-Control": "no-cache"}
    
            for attempt in range(max_retries):
                # Check if DAG exists
                print("Checking if the DAG exists...")
                dag_response = requests.get(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/sales_profit_pipeline",
                    auth=auth,
                    headers=headers,
                )
    
                if dag_response.status_code != 200:
                    if attempt == max_retries - 1:
                        raise Exception("DAG not found after max retries")
                    print(f"DAG not found, attempt {attempt + 1} of {max_retries}")
                    time.sleep(10)
                    continue
    
                print("Unpausing the DAG...")
                # Unpause the DAG before triggering
                unpause_response = requests.patch(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/sales_profit_pipeline",
                    auth=auth,
                    headers=headers,
                    json={"is_paused": False},
                )
    
                if unpause_response.status_code != 200:
                    if attempt == max_retries - 1:
                        raise Exception(f"Failed to unpause DAG: {unpause_response.text}")
                    print(f"Failed to unpause DAG, attempt {attempt + 1} of {max_retries}")
                    time.sleep(10)
                    continue
    
                print("Triggering the DAG...")
                # Trigger the DAG
                trigger_response = requests.post(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/sales_profit_pipeline/dagRuns",
                    auth=auth,
                    headers=headers,
                    json={"conf": {}},
                )
    
                if trigger_response.status_code == 200:
                    dag_run_id = trigger_response.json()["dag_run_id"]
                    print("DAG triggered successfully")
                    break
                else:
                    if attempt == max_retries - 1:
                        raise Exception(f"Failed to trigger DAG: {trigger_response.text}")
                    print(f"Failed to trigger DAG, attempt {attempt + 1} of {max_retries}")
                    time.sleep(10)
    
            # Monitor the DAG run
            print("Monitoring the DAG run...")
            max_wait = 300  # 5 minutes timeout
            start_time = time.time()
            while time.time() - start_time < max_wait:
                status_response = requests.get(
                    f"{airflow_base_url.rstrip('/')}/api/v1/dags/sales_profit_pipeline/dagRuns/{dag_run_id}",
                    auth=auth,
                    headers=headers,
                )
    
                if status_response.status_code == 200:
                    state = status_response.json()["state"]
                    print(f"DAG state: {state}")
                    if state == "success":
                        print("DAG completed successfully")
                        break
                    elif state in ["failed", "error"]:
                        raise Exception(f"DAG failed with state: {state}")
    
                time.sleep(10)
            else:
                raise Exception("DAG run timed out")
    
            # SECTION 3: VERIFY THE OUTCOMES
            print("Verifying the outcomes...")
            # Verify data in MySQL
            mysql_cursor.execute(
                """
                SELECT * FROM daily_profits
                WHERE date = '2024-01-01'
                ORDER BY user_id
            """
            )
    
            results = mysql_cursor.fetchall()
>           assert len(results) == 2, "Expected 2 records in daily_profits"
E           AssertionError: Expected 2 records in daily_profits
E           assert 0 == 2
E            +  where 0 = len([])

Tests/Postgres_to_MySQL_Pipeline/test_postgres_to_mysql_pipeline.py:398: AssertionError
_________________________ test_simple_airflow_pipeline _________________________

request = <FixtureRequest for <Function test_simple_airflow_pipeline>>

    @pytest.mark.airflow
    @pytest.mark.pipeline
    def test_simple_airflow_pipeline(request):
        input_dir = os.path.dirname(os.path.abspath(__file__))
        request.node.user_properties.append(("user_query", Test_Configs.User_Input))
        airflow_local = Airflow_Local()
    
    
        test_steps = [
            {
                "name": "Checking Git Branch Existence",
                "description": "Checking if the git branch exists with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking PR Creation",
                "description": "Checking if the PR was created with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking Dag Results",
                "description": "Checking if the DAG produces the expected results",
                "status": "did not reach",
                "Result_Message": "",
            },
        ]
    
        request.node.user_properties.append(("test_steps", test_steps))
    
        # SECTION 1: SETUP THE TEST
        config_results = None  # Initialize before try block
        try:
            # Setup GitHub repository with empty dags folder
            access_token = os.getenv("AIRFLOW_GITHUB_TOKEN")
            airflow_github_repo = os.getenv("AIRFLOW_REPO")
    
            # Convert full URL to owner/repo format if needed
            if "github.com" in airflow_github_repo:
                # Extract owner/repo from URL
                parts = airflow_github_repo.split("/")
                airflow_github_repo = f"{parts[-2]}/{parts[-1]}"
    
            g = Github(access_token)
            repo = g.get_repo(airflow_github_repo)
    
            try:
                # First, clear only the dags folder
                dags_contents = repo.get_contents("dags")
                for content in dags_contents:
                    if content.name != ".gitkeep":  # Keep the .gitkeep file if it exists
                        repo.delete_file(
                            path=content.path,
                            message="Clear dags folder",
                            sha=content.sha,
                            branch="main",
                        )
    
                # Ensure .gitkeep exists in dags folder
                try:
                    repo.get_contents("dags/.gitkeep")
                except:
                    repo.create_file(
                        path="dags/.gitkeep",
                        message="Add .gitkeep to dags folder",
                        content="",
                        branch="main",
                    )
            except Exception as e:
                if "sha" not in str(e):  # If error is not about folder already existing
                    raise e
    
            # set the airflow folder with the correct configs
    
            # this function is for you to take the configs for the test and set them up however you want. They follow a set structure
            config_results = set_up_model_configs(Configs=Test_Configs.Configs)
    
            # SECTION 2: RUN THE MODEL
            start_time = time.time()
            run_model(
                container=None, task=Test_Configs.User_Input, configs=Test_Configs.Configs
            )
            end_time = time.time()
            request.node.user_properties.append(("model_runtime", end_time - start_time))
    
            # Check if the branch exists
            try:
>               branch = repo.get_branch("feature/hello_world_dag")

Tests/Simple_Airflow_Pipeline/test_simple_airflow_pipeline.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
dev_venv/lib/python3.11/site-packages/github/Repository.py:2056: in get_branch
    headers, data = self._requester.requestJsonAndCheck("GET", f"{self.url}/branches/{branch}")
dev_venv/lib/python3.11/site-packages/github/Requester.py:586: in requestJsonAndCheck
    return self.__check(*self.requestJson(verb, url, parameters, headers, input, self.__customConnection(url)))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <github.Requester.Requester object at 0x10fbfaf50>, status = 404
responseHeaders = {'access-control-allow-origin': '*', 'access-control-expose-headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP...Hub-Request-Id, Deprecation, Sunset', 'content-encoding': 'gzip', 'content-security-policy': "default-src 'none'", ...}
output = '{"message":"Branch not found","documentation_url":"https://docs.github.com/rest/branches/branches#get-a-branch","status":"404"}'

    def __check(
        self,
        status: int,
        responseHeaders: Dict[str, Any],
        output: str,
    ) -> Tuple[Dict[str, Any], Any]:
        data = self.__structuredFromJson(output)
        if status >= 400:
>           raise self.createException(status, responseHeaders, data)
E           github.GithubException.GithubException: 404 {"message": "Branch not found", "documentation_url": "https://docs.github.com/rest/branches/branches#get-a-branch", "status": "404"}

dev_venv/lib/python3.11/site-packages/github/Requester.py:744: GithubException

During handling of the above exception, another exception occurred:

request = <FixtureRequest for <Function test_simple_airflow_pipeline>>

    @pytest.mark.airflow
    @pytest.mark.pipeline
    def test_simple_airflow_pipeline(request):
        input_dir = os.path.dirname(os.path.abspath(__file__))
        request.node.user_properties.append(("user_query", Test_Configs.User_Input))
        airflow_local = Airflow_Local()
    
    
        test_steps = [
            {
                "name": "Checking Git Branch Existence",
                "description": "Checking if the git branch exists with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking PR Creation",
                "description": "Checking if the PR was created with the right name",
                "status": "did not reach",
                "Result_Message": "",
            },
            {
                "name": "Checking Dag Results",
                "description": "Checking if the DAG produces the expected results",
                "status": "did not reach",
                "Result_Message": "",
            },
        ]
    
        request.node.user_properties.append(("test_steps", test_steps))
    
        # SECTION 1: SETUP THE TEST
        config_results = None  # Initialize before try block
        try:
            # Setup GitHub repository with empty dags folder
            access_token = os.getenv("AIRFLOW_GITHUB_TOKEN")
            airflow_github_repo = os.getenv("AIRFLOW_REPO")
    
            # Convert full URL to owner/repo format if needed
            if "github.com" in airflow_github_repo:
                # Extract owner/repo from URL
                parts = airflow_github_repo.split("/")
                airflow_github_repo = f"{parts[-2]}/{parts[-1]}"
    
            g = Github(access_token)
            repo = g.get_repo(airflow_github_repo)
    
            try:
                # First, clear only the dags folder
                dags_contents = repo.get_contents("dags")
                for content in dags_contents:
                    if content.name != ".gitkeep":  # Keep the .gitkeep file if it exists
                        repo.delete_file(
                            path=content.path,
                            message="Clear dags folder",
                            sha=content.sha,
                            branch="main",
                        )
    
                # Ensure .gitkeep exists in dags folder
                try:
                    repo.get_contents("dags/.gitkeep")
                except:
                    repo.create_file(
                        path="dags/.gitkeep",
                        message="Add .gitkeep to dags folder",
                        content="",
                        branch="main",
                    )
            except Exception as e:
                if "sha" not in str(e):  # If error is not about folder already existing
                    raise e
    
            # set the airflow folder with the correct configs
    
            # this function is for you to take the configs for the test and set them up however you want. They follow a set structure
            config_results = set_up_model_configs(Configs=Test_Configs.Configs)
    
            # SECTION 2: RUN THE MODEL
            start_time = time.time()
            run_model(
                container=None, task=Test_Configs.User_Input, configs=Test_Configs.Configs
            )
            end_time = time.time()
            request.node.user_properties.append(("model_runtime", end_time - start_time))
    
            # Check if the branch exists
            try:
                branch = repo.get_branch("feature/hello_world_dag")
                test_steps[0]["status"] = "passed"
                test_steps[0][
                    "Result_Message"
                ] = "Branch 'feature/hello_world_dag' was created successfully"
            except Exception as e:
                test_steps[0]["status"] = "failed"
                test_steps[0][
                    "Result_Message"
                ] = f"Branch 'feature/hello_world_dag' was not created: {str(e)}"
>               raise Exception(
                    f"Branch 'feature/hello_world_dag' was not created: {str(e)}"
                )
E               Exception: Branch 'feature/hello_world_dag' was not created: 404 {"message": "Branch not found", "documentation_url": "https://docs.github.com/rest/branches/branches#get-a-branch", "status": "404"}

Tests/Simple_Airflow_Pipeline/test_simple_airflow_pipeline.py:120: Exception
=============================== warnings summary ===============================
Tests/Amazon_SP_API_To_Postgres/test_amazon_sp_api_to_postgres.py:24
  /Users/vikram/Documents/ArdentAI/DE-Bench/Tests/Amazon_SP_API_To_Postgres/test_amazon_sp_api_to_postgres.py:24: PytestUnknownMarkWarning: Unknown pytest.mark.amazon_sp_api - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.amazon_sp_api

Tests/Amazon_SP_API_To_Postgres/test_amazon_sp_api_to_postgres.py::test_amazon_sp_api_to_postgres
Tests/MySQL_to_tigerbeetle/test_mysql_to_tigerbeetle.py::test_mysql_to_tigerbeetle
Tests/Postgres_to_MySQL_Pipeline/test_postgres_to_mysql_pipeline.py::test_postgres_to_mysql_pipeline
Tests/Simple_Airflow_Pipeline/test_simple_airflow_pipeline.py::test_simple_airflow_pipeline
  /Users/vikram/Documents/ArdentAI/DE-Bench/dev_venv/lib/python3.11/site-packages/github/MainClass.py:228: DeprecationWarning: Argument login_or_token is deprecated, please use auth=github.Auth.Token(...) instead
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED Tests/Amazon_SP_API_To_Postgres/test_amazon_sp_api_to_postgres.py::test_amazon_sp_api_to_postgres
FAILED Tests/MySQL_to_tigerbeetle/test_mysql_to_tigerbeetle.py::test_mysql_to_tigerbeetle
FAILED Tests/Postgres_to_MySQL_Pipeline/test_postgres_to_mysql_pipeline.py::test_postgres_to_mysql_pipeline
FAILED Tests/Simple_Airflow_Pipeline/test_simple_airflow_pipeline.py::test_simple_airflow_pipeline
============= 4 failed, 1 passed, 5 warnings in 1196.62s (0:19:56) =============
